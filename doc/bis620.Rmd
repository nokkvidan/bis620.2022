---
title: "bis620"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bis620}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#"
  # root.dir = "~/bis620.2022/"
)
```

```{r setup}
library(bis620.2022)
library(dplyr)
library(randomForest)

# Load the data
# devtools::install_github("nokkvidan/bis620.2022")
data(diabetes)

# Splitting the dataset for train and test
set.seed(1)
sample <- sample(c(1, 2), nrow(diabetes), replace = TRUE, prob = c(.8, .2))

# convert all variables to categorical, except for Age and BMI
train <- diabetes[sample == 1, ] %>%
  select(-c(4, 13)) %>%
  mutate_all(as.factor)
train[, c("Age", "BMI")] <- diabetes[sample == 1, c("Age", "BMI")]

test <- diabetes[sample == 2, ] %>%
  select(-c(4, 13)) %>%
  mutate_all(as.factor)
test[, c("Age", "BMI")] <- diabetes[sample == 2, c("Age", "BMI")]
```

# Motivations

It is estimated that 1 in 10 people in America have diabetes and 1 in 5 of those with the health condition have not been clinically diagnosed [1]. Type 1 diabetes occurs when the body is unable to produce insulin and is inherited from their family, but type 2 diabetes occurs when the body is unable to use insulin properly and can be prevented or delayed with improvements to a patient's lifestyle.

Our goal is to identify what kinds of health indicators might be linked with Type 2 diabetes and pre-diabetes.

By using this project, healthcare professionals will be able to quickly determine the likelihood of a patient's susceptibility to diabetes, allowing for earlier detection and providing guidelines to inform lifestyle changes and treatments. This can ultimately improve individual patient outcomes and reduce the overall proportion of diabetes in the population.

Additionally, this project can help researchers study the significant correlation between specific health indicators and the likelihood of developing diabetes. By using the functions provided in this project, researchers can quickly analyze the relationship between different variables and diabetes, potentially uncovering new insights into the causes and risk factors of the condition.

To accomplish this goal, we developed this diabetes risk prediction R package which helps facilitate the analysis of health indicators such as age, BMI, and sex. Through a number of machine learning algorithms we can analyze a large Kaggle data set of patients with known diabetes status and health indicators [2]. Theses algorithms use a combination of continuous and categorical variables to create a model that can predict the likelihood of a patient having diabetes. The following models are available in this package as functions: generalized linear model (logistic regression), XGBoost and random forests. These functions also include additional options for optimization and evaluation.

The data set `diabetes` has been attached to the package and is available to use as demonstrated with the examples below.

# About the package functions

## EDA functions

The diabetes risk prediction R project also includes a suite of visualization functions that allow for the exploratory analysis of the data set used to train the prediction model. These functions provide an interactive and intuitive way to explore the relationship between different variables and the likelihood of a patient having diabetes.

#### Visualize interactions (2 variables)

Using the function `vis_2vars()` we can visualize the interaction of any two variables in the data set. The function determines what type the variables are and then produces a plot accordingly. The options are bar-, violin-, and scatter plots depending on the kinds of variables that are put into the function.

```{r vis2, warnings = FALSE, fig.width=7, fig.height=7}
# Factor v continuous produces a violin plot
# Continuous v factor also produces a violin plot
p1 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
# Continuous v continuous produces a scatter plot
p2 <- vis_2vars(diabetes, "MentHlth", "BMI")
# Factor v factor produces a side by side bar plot
p3 <- vis_2vars(diabetes, "Diabetes_binary", "Sex")
p4 <- vis_2vars(diabetes, "Age", "Diabetes_binary")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

#### Visualise numeric variables

Using the function `vis_num()` we can get an overview of the numeric variables in our data set and how they could potentially used if scaled to the same scale of $[0,1]$. Thus, this function visualizes all the non-categorical variables in the data set with boxplots.

```{r visnum, fig.width=7}
p5 <- vis_num(diabetes)
```

#### Visualise distributions of numeric variables

Using the function `vis_dist()` we see more detailed distribution plots of the numeric variables that we want to investigate. The plot includes a qqplot and a histogram of the numeric variables in the data set provided.

From the plot above we might want to consider visualizing `BMI` and `MentHlth` as they are a bit skewed. We also add `Sex` into the parameters to show that function properly removes factor variables.

```{r visdist, fig.width=7, fig.height = 6}
df_dist <- diabetes[, c("BMI", "MentHlth", "Sex")]
vis_dist(df_dist)
```

#### Summary of EDA functions

Overall, these visualization functions provide a powerful tool for the exploration and analysis of the data set, and can help researchers gain a better understanding of the factors that influence the likelihood of a patient having diabetes.

## Summary of modeling and evaluation functions

The R package allows health care professionals to easily model and evaluate generalized linear models (GLM) logits, XGBoost, and random forests for identifying significant variables correlated with diabetes. With these functions, users can quickly fit these models to their data and assess the performance of each model to determine the best approach for their specific use case. To be able to evaluate the performance one must split the data set into training and test sets (shown above).

In addition, the package offers a range of evaluation metrics and visualization tools to help users interpret the results of the model and make informed decisions about the factors associated with diabetes. Overall, the functions provide a convenient and user-friendly solution for health care professionals looking to use machine learning methods to analyze diabetes data.

```{r, eval = FALSE}
# Fit a GLM to the diabetes data
glm_fit0 <- glm_model("Diabetes_binary", diabetes)
# Fit a random forest model to the diabetes data
rf_fit0 <- rf_model("Diabetes_binary", diabetes)
# Fit an XGBoost model to the diabetes data
xg_fit0 <- boost_model("Diabetes_binary", diabetes)
```

``` {r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
# Fit and evaluate the performance of the GLM model
glm_fit <- glm_model("Diabetes_binary", train, test, optimize = "manual")
# Fit and evaluate the performance of the random forest model
rf_fit <- rf_model("Diabetes_binary", train, test, optimize = TRUE)
# Fit and evaluate the performance of the XGBoost model
xg_fit <- boost_model("Diabetes_binary", train, test)
```

# Modeling analysis

## Logistic regression

According to the optimized logistic regression, we can see that several factors were marked as important in predicting diabetes. For example, a high blood pressure, high cholesterol, heart disease or attack, amongst others, though these had a high probability for the t-test removing the variable. Interestingly, we can see that all of the higher incomes were significant and the extent to which the significance increased with income. This suggests that income appears to scale inversely with the chance of type 2 diabetes. Another surprising figure is the coefficient for heavy alcohol consumption, -0.75. This coefficient is negative which means that patients who heavily consume alcohol are less likely to have been diagnosed with diabetes/pre-diabetes. A possible explanation for this could be that the variable is linked to sex and future upgrades to this package could include the option to add interaction variables.

The rest of the coefficient signs are fairly expected. The indicators for high blood pressure, high cholesterol, larger BMI, strokes, heart disease/attacks, and general health (larger value = worse health) contribute to diabetes. At the same time, eating veggies, having strong physical health and more education have negative coefficients contributing to decreasing in the likelihood of having diabetes.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
glm_fit <- glm_model("Diabetes_binary", diabetes, optimize = "manual")
```

```{r}
glm_fit <- glm_model("Diabetes_binary", train, test, optimize = "manual")
summary(glm_fit)
```

The following plots show three variables which were marked as highly significant, along with the BMI. The plot makes it clear that those with diabetes have a larger proportion of people with high blood pressure and high cholesterol than those without diabetes. Additionally, there are more men with diabetes than without and less women with diabetes than without. Their differences; however, are not very large and so it makes sense that the coefficient for sex in our logistic regression is not very large.

```{r, fig.width=7, fig.height=7}
p1 <- vis_2vars(diabetes, "Diabetes_binary", "HighBP")
p2 <- vis_2vars(diabetes, "Diabetes_binary", "HighChol")
p4 <- vis_2vars(diabetes, "Sex", "Diabetes_binary")
p3 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

## Random Forest

We ran the random forest model on the full data and chosen to optimize the model which means that we optimized the model to have the best mtry value. In other words, we chose the best tree split based on a random sample of an mtry number of predictors. Here that number is optimized to be 2 and the number of trees is 100. The error rate is 25.58% which means that it has 74.42% accuracy. For this model, we split the data into train and test and evaluated its performance. The accuracy for the test data was similar to that of the model, 74.84 %, which is not high for either case and the confidence interval is from 73.98% to 75.42%.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
rf_fit <- rf_model("Diabetes_binary", train, test, optimize = TRUE)
```

```{r}
print(rf_fit[[1]])
rf_fit[[6]]
```

The random forest model evaluated (according to the mean decrease in accuracy metric) BMI (30.82), general health (26.49), Age (24.75), blood pressure (22.16), and cholesterol (22.13) as the top five most important variables for categorizing patients between having and not having diabetes. It is unsurprising that both models would tie diabetes to general health and blood pressure as being important indicators similar to the glm. Meanwhile, the values that are least important are smoking (2.05), mental health (2.86), fruits (3.34), and veggies (3.22). This was also similar to the results from the previous model, where mental health had a small coefficient and both fruits and smoking were not significant enough to be included in the model.

The area under the curve here is 0.8186, which is not terrible and means that our model has a good level of separability between classes since it is close to 1.

```{r}
importance(rf_fit[[1]])
rf_fit[[7]]
```

## XG Boost
The training logloss is roughly 0.14 which is numerically small, however given that our predicted variable range is 0 to 1, it is relatively large for this model. The accuracy of 70.77% which means that both statistics agree the model is okay but not great at predicting diabetes.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
xg_fit <- boost_model("Diabetes_binary", train, test)
```

```{r}
print(xg_fit[[1]])
print(xg_fit[[2]])
rf_fit[[7]]
```

Similar to the first two models, BMI(gain/contribution to the model):0.179, blood pressure: 0.176, age: 0.122, high cholesterol: 0.037 and general health: 0.028, are all important factors in predicting diabetes. Unlike the other models; however, age: 0.122 now seems to be higher in importance. This variable is unfortunately non-actionable in terms of lifestyle changes.
```{r}
print(xg_fit[[3]])
```


In conclusion, the most powerful model of our three variables is the xgboost. Variables like the doctor’s cost, fruits and veggies aren’t indicators that are as important as BMI, general health, blood pressure and cholesterol across models when predicting for diabetes. Also, variables like strokes and heart disease/attacks flip between the most and the least important depending on the model.

# Counter Factuals

We note that each of the three models had their own way of assessing feature relevance. We see that the glm is the easiest to interpret, the random forests should be the most robust against outliers and that xgboost is the most powerful. We attempted to find a set of variables that were marked as significant by each of the models (according to their own metrics), or at the least, variables in which the majority of models marked as significant. We also selected variables that a patient could take an action to change, or reduce the risk of it occurring, for example the patient could lower the BMI through exercise. This was so that the doctor could give the patient a concrete set of improvements to their lives to reduce the likelihood of diabetes.

We proceeded by initially fitting our model of the subset of the test data, that were diabetic and which belonged to the class of our specified variable. For example, diabetic patients which had a “high cholesterol” value of 1. The variables we ended up choosing based on these criteria were blood pressure, cholesterol, BMI and heart disease/attack. All of these, except for BMI are categorical variables.

We would examine the initial prediction for the proportion of individuals we predict as having diabetes for our model. We would then change the value of the variable (the counterfactual), to a healthy value (or range in the case of BMI) and notice the increase/decrease in the proportion of individuals being classified as having diabetes.

In each of the models, performing the counterfactual change reduced the proportion of individuals being classified as having type 2 diabetes for all of our variables. Changing the BMI had the greatest effect in 2 of our models and the 2nd biggest effect in the other. So, it seems the best action to take would be to reduce a patients BMI. This is then followed by reducing the blood pressure and cholesterol levels, which the doctor could also provide remedies for. It seems that the heart disease or attack had consistently the smallest decreases amongst all models so should be given least priority amongst them.

We output the analysis referenced above in the tables below:

Note: that only variables that the glm considers significant could be used for our counterfactual model

```{r}
# xg boost counterfactual
eval_xg(train, test, xg_fit)
```

```{r}
# random forest counterfactual
eval_rf_and_glm(train, test, rf_fit, type = "prob")
```

```{r}
# glm counterfactual
eval_rf_and_glm(train, test, glm_fit, type = "response")
```