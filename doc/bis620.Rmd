---
title: "bis620"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bis620}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#"
)
```

```{r setup}
library(bis620.2022)
library(randomForest)

# Load the data
data(diabetes)

# Splitting the dataset for train and test
set.seed(1)
sample <- sample(c(1, 2), nrow(diabetes), replace = TRUE, prob = c(.8, .2))
train <- diabetes[sample == 1, ]
test <- diabetes[sample == 2, ]
```

# Motivations

It is estimated that 1 in 10 people in America have diabetes and 1 in 5 of those with the health condition have not been clinically diagnosed [1]. Type 1 diabetes occurs when the body is unable to produce insulin and is inherited from their family, but type 2 diabetes occurs when the body is unable to use insulin properly and can be prevented or delayed with improvements to a patient's lifestyle. 

Our goal is to identify what kinds of health indicators might be linked with Type 2 diabetes and pre-diabetes.

By using this project, healthcare professionals will be able to quickly determine the likelihood of a patient's susceptibility to diabetes, allowing for earlier detection and providing guidelines to inform lifestyle changes and treatments. This can ultimately improve individual patient outcomes and reduce the overall proportion of diabetes in the population.

Additionally, this project can help researchers study the significant correlation between specific health indicators and the likelihood of developing diabetes. By using the functions provided in this project, researchers can quickly analyze the relationship between different variables and diabetes, potentially uncovering new insights into the causes and risk factors of the condition.

To accomplish this goal, we developed this diabetes risk prediction R package which helps facilitate the analysis of health indicators such as age, BMI, and sex. Through a number of machine learning algorithms we can analyze a large Kaggle data set of patients with known diabetes status and health indicators [2]. Theses algorithms use a combination of continuous and categorical variables to create a model that can predict the likelihood of a patient having diabetes. The following models are available in this package as functions: generalized linear model (logistic regression), XGBoost and random forests. These functions also include additional options for optimization and evaluation.

The data set `diabetes` has been attached to the package and is available to use as demonstrated with the examples below.

# About the package functions
## EDA functions
The diabetes risk prediction R project also includes a suite of visualization functions that allow for the exploratory analysis of the data set used to train the prediction model. These functions provide an interactive and intuitive way to explore the relationship between different variables and the likelihood of a patient having diabetes.

#### Visualize interactions (2 variables)

Using the function `vis_2vars()` we can visualize the interaction of any two variables in the data set. The function determines what type the variables are and then produces a plot accordingly. The options are bar-, violin-, and scatter plots depending on the kinds of variables that are put into the function.

```{r vis2, warnings = FALSE, fig.width=7, fig.height=7}
# Factor v continuous produces a violin plot
# Continuous v factor also produces a violin plot
p1 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
# Continuous v continuous produces a scatter plot
p2 <- vis_2vars(diabetes, "MentHlth", "BMI")
# Factor v factor produces a side by side bar plot
p3 <- vis_2vars(diabetes, "Diabetes_binary", "Sex")
p4 <- vis_2vars(diabetes, "Age", "Diabetes_binary")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

#### Visualise numeric variables

Using the function `vis_num()` we can get an overview of the numeric variables in our data set and how they could potentially used if scaled to the same scale of $[0,1]$. Thus, this function visualizes all the non-categorical variables in the data set with boxplots.

```{r visnum, fig.width=7}
p5 <- vis_num(diabetes)
```

#### Visualise distributions of numeric variables

Using the function `vis_dist()` we see more detailed distribution plots of the numeric variables that we want to investigate. The plot includes a qqplot and a histogram of the numeric variables in the data set provided.

From the plot above we might want to consider visualizing `BMI` and `MentHlth` as they are a bit skewed. We also add `Sex` into the parameters to show that function properly removes factor variables.

```{r visdist, fig.width=7, fig.height = 6}
df_dist <- diabetes[, c("BMI", "MentHlth", "Sex")]
vis_dist(df_dist)
```

#### Summary of EDA functions

Overall, these visualization functions provide a powerful tool for the exploration and analysis of the data set, and can help researchers gain a better understanding of the factors that influence the likelihood of a patient having diabetes.

## Summary of modeling and evaluation functions

The R package allows health care professionals to easily model and evaluate generalized linear models (GLM) logits, XGBoost, and random forests for identifying significant variables correlated with diabetes. With these functions, users can quickly fit these models to their data and assess the performance of each model to determine the best approach for their specific use case. To be able to evaluate the performance one must split the data set into training and test sets (shown above).

In addition, the package offers a range of evaluation metrics and visualization tools to help users interpret the results of the model and make informed decisions about the factors associated with diabetes. Overall, the functions provide a convenient and user-friendly solution for health care professionals looking to use machine learning methods to analyze diabetes data.

```{r, eval = FALSE}
# Fit a GLM to the diabetes data
glm_fit0 <- glm_model("Diabetes_binary", diabetes)
# Fit a random forest model to the diabetes data
rf_fit0 <- rf_model("Diabetes_binary", diabetes)
# Fit an XGBoost model to the diabetes data
xg_fit0 <- boost_model("Diabetes_binary", diabetes)
```

``` {r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
# Fit and evaluate the performance of the GLM model
glm_fit <- glm_model("Diabetes_binary", train, test, optimize = "manual")
# Fit and evaluate the performance of the random forest model
rf_fit <- rf_model("Diabetes_binary", train, test, optimize = TRUE)
# Fit and evaluate the performance of the XGBoost model
xg_fit <- boost_model("Diabetes_binary", train, test)
```


# Modeling analysis
## Logistic regression
According to the optimized logistic regression, the factors that contribute to the probability of having diabetes are high blood pressure(0.75), high cholesterol (0.6), and general health (0.59). 
BMI's coefficient, 0.08, is not as strong as would have been expected, but the distribution between the two is fairly similar. Another surprising figure is the coefficient for heavy alcohol consumption, -0.75. This coefficient is negative which means that patients who heavily consume alcohol are less likely to have been diagnosed with diabetes/pre-diabetes. A possible explanation for this could be that the variable is linked to sex and future upgrades to this package could include the option to add interaction variables.

The rest of the coefficient signs are fairly expected. The indicators for high blood pressure, high cholesterol, larger BMI, strokes, heart disease/attacks, and general health (larger value = worse health) contribute to diabetes. At the same time, eating veggies, having strong physical health, more income, and more education have negative coefficients contributing to decreasing in the likelihood of having diabetes. An increase in number of days with mental health concerns does have a negative coefficient, -0.005, and there is not a clear reason for why this might be the case.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
glm_fit <- glm_model("Diabetes_binary", diabetes[, -c(4,13)], optimize = "manual")
```

```{r}
summary(glm_fit)
```

The following plots show the three variables with the largest coefficients as well as BMI. The plot makes it clear that those with diabetes have a larger proportion of people with high blood pressure and high cholesterol than those without diabetes. Additionally, there are more men with diabetes than without and less women with diabetes than without. Their differences; however, are not very large and so it makes sense that the coefficient for sex in our logistic regression is not very large. 


```{r, fig.width=7, fig.height=7}
p1 <- vis_2vars(diabetes, "Diabetes_binary", "HighBP")
p2 <- vis_2vars(diabetes, "Diabetes_binary", "HighChol")
p4 <- vis_2vars(diabetes, "Sex", "Diabetes_binary")
p3 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

## Random Forest
We ran the random forest model on the full data and chosen to optimize the model which means that we optimized the model to have the best mtry value. In other words, we chose the best tree split based on a random sample of an mtry number of predictors. Here that number is optimized to be 2 and the number of trees is 100. The error rate is 25.48% which means that it has 74.52% accuracy. For this model, we split the data into train and test and evaluated its performance. The accuracy for the test data was similar to that of the model, 74.67%, which is not high for either case and the confidence interval is from 73.94% to 75.38%

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
rf_fit <- rf_model("Diabetes_binary", train[, -c(4,13)], test[, -c(4,13)], optimize = TRUE)
```

```{r}
print(rf_fit[[1]])
rf_fit[[6]]
```

The random forest model evaluated BMI (25.12), general health (18.22), heart disease/attacks (17.79), blood pressure (17.17), and cholesterol (16.50) as the top five most important variables for categorizing patients between having and not having diabetes. It is unsurprising that both models would tie diabetes to general health and blood pressure as being important indicators similar to the glm. Meanwhile, the values that are least important are sex (-0.17), mental health (1.99), fruits (3.56), smoking (3.59), and veggies (5.40). This was also similar to the results from the previous model, where mental health had a small coefficient and both fruits and smoking were not significant enough to be included in the model. 

The area under the curve here is 0.8193, which is not terrible and means that our model has a good level of separability between classes since it is close to 1. 

```{r}
importance(rf_fit[[1]])
rf_fit[[7]]
```

## XG Boost
This final model, XG Boost, is the most powerful of our three models. The training RMSE is roughly 0.12 which is numerically small, however given that our predicted variable range is 0 to 1, it is relatively large for this model. The accuracy of 70.14% which means that both statistics agree the model is okay but not great at predicting diabetes. 

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
xg_fit <- boost_model("Diabetes_binary", train[, -c(4,13)], test[, -c(4,13)])
```

```{r}
print(xg_fit[[1]])
print(xg_fit[[2]])
rf_fit[[7]]
```

Similar to the first two models, blood pressure (gain/contribution to the model: 0.16), BMI (0.15), and general health (0.12) are all important factors in predicting diabetes. Unlike the other models; however, age (0.10) now seems to be higher in importance. This variable is unfortunately non-actionable in terms of lifestyle changes. 
Variables that do not contribute as much to the model are heavy alcohol consumption (0.010), strokes (0.010), and hesitation to visit doctor due to cost (0.012). 
```{r}
print(xg_fit[[3]])
```


In conclusion, variables like to doctor's cost, fruits and veggies aren't indicators that are as important as BMI, general health, blood pressure, and cholesterol across models when predicting for diabetes. Also, variables like strokes and heart disease/attacks flip between the most and the least important depending on the model.


# Counter Factuals


