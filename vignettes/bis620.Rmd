---
title: "bis620"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bis620}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#"
)
```

```{r setup, message = FALSE}
library(bis620.2022)
library(dplyr, include.only = c("mutate_at", "%>%", "vars"))

# Load the data
data(diabetes)

# Splitting the dataset for train and test
set.seed(1)
sample <- sample(c(1, 2), nrow(diabetes), replace = TRUE, prob = c(.8, .2))

# convert all variables to categorical, except for Age and BMI
train <- diabetes[sample == 1, ] %>%
  mutate_at(vars(-c("Age", "BMI")), as.factor)
test <- diabetes[sample == 2, ] %>%
  mutate_at(vars(-c("Age", "BMI")), as.factor)
```

# Motivations

It is estimated that 1 in 10 people in America have diabetes and 1 in 5 of those with the health condition have not been clinically diagnosed [1]. Type 1 diabetes occurs when the body is unable to produce insulin and is inherited from their family, but type 2 diabetes occurs when the body is unable to use insulin properly and can be prevented or delayed with improvements to a patient's lifestyle.

Our goal is to identify what kinds of health indicators might be linked with Type 2 diabetes and pre-diabetes.

By using this project, healthcare professionals will be able to quickly determine the likelihood of a patient's susceptibility to diabetes, allowing for earlier detection and providing guidelines to inform lifestyle changes and treatments. This can ultimately improve individual patient outcomes and reduce the overall proportion of diabetes in the population.

Additionally, this project can help researchers study the significant correlation between specific health indicators and the likelihood of developing diabetes. By using the functions provided in this project, researchers can quickly analyze the relationship between different variables and diabetes, potentially uncovering new insights into the causes and risk factors of the condition.

To accomplish this goal, we developed this diabetes risk prediction R package which helps facilitate the analysis of health indicators such as age, BMI, and sex. Through a number of machine learning algorithms we can analyze a large Kaggle data set of patients with known diabetes status and health indicators [2]. Theses algorithms use a combination of continuous and categorical variables to create a model that can predict the likelihood of a patient having diabetes. The following models are available in this package as functions: generalized linear model (logistic regression), XGBoost and random forests. These functions also include additional options for optimization and evaluation.

The data set `diabetes` has been attached to the package and is available to use as demonstrated with the examples below.

# About the package functions

## EDA functions

The diabetes risk prediction R project also includes a suite of visualization functions that allow for the exploratory analysis of the data set used to train the prediction model. These functions provide an interactive and intuitive way to explore the relationship between different variables and the likelihood of a patient having diabetes.

#### Visualize interactions (2 variables)

Using the function `vis_2vars()` we can visualize the interaction of any two variables in the data set. The function determines what type the variables are and then produces a plot accordingly. The options are bar-, violin-, and scatter plots depending on the kinds of variables that are put into the function.

```{r vis2, warnings = FALSE, fig.width=7, fig.height=7}
# Factor v continuous produces a violin plot
# Continuous v factor also produces a violin plot
p1 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
# Continuous v continuous produces a scatter plot
p2 <- vis_2vars(diabetes, "MentHlth", "BMI")
# Factor v factor produces a side by side bar plot
p3 <- vis_2vars(diabetes, "Diabetes_binary", "Sex")
p4 <- vis_2vars(diabetes, "Age", "Diabetes_binary")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

#### Visualise numeric variables

Using the function `vis_num()` we can get an overview of the numeric variables in our data set and how they could potentially used if scaled to the same scale of $[0,1]$. Thus, this function visualizes all the non-categorical variables in the data set with boxplots.

```{r visnum, fig.width=7}
p5 <- vis_num(diabetes)
```

#### Visualise distributions of numeric variables

Using the function `vis_dist()` we see more detailed distribution plots of the numeric variables that we want to investigate. The plot includes a qqplot and a histogram of the numeric variables in the data set provided.

From the plot above we might want to consider visualizing `BMI` and `MentHlth` as they are a bit skewed. We also add `Sex` into the parameters to show that function properly removes factor variables.

```{r visdist, fig.width=7, fig.height = 6}
df_dist <- diabetes[, c("BMI", "MentHlth", "Sex")]
vis_dist(df_dist)
```

#### Summary of EDA functions

Overall, these visualization functions provide a powerful tool for the exploration and analysis of the data set, and can help researchers gain a better understanding of the factors that influence the likelihood of a patient having diabetes.

## Summary of modeling and evaluation functions

The R package allows health care professionals to easily model and evaluate generalized linear models (GLM) logits, XGBoost, and random forests for identifying significant variables correlated with diabetes. With these functions, users can quickly fit these models to their data and assess the performance of each model to determine the best approach for their specific use case. To be able to evaluate the performance one must split the data set into training and test sets (shown above).

In addition, the package offers a range of evaluation metrics and visualization tools to help users interpret the results of the model and make informed decisions about the factors associated with diabetes. Overall, the functions provide a convenient and user-friendly solution for health care professionals looking to use machine learning methods to analyze diabetes data.

```{r, eval = FALSE}
# Fit a GLM to the diabetes data
glm_fit0 <- glm_model("Diabetes_binary", diabetes)
# Fit a random forest model to the diabetes data
rf_fit0 <- rf_model("Diabetes_binary", diabetes)
# Fit an XGBoost model to the diabetes data
xg_fit0 <- boost_model("Diabetes_binary", diabetes)
```

``` {r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
# Fit and evaluate the performance of the GLM model
glm_fit <- glm_model("Diabetes_binary", train, test, optimize = "manual")
# Fit and evaluate the performance of the random forest model
rf_fit <- rf_model("Diabetes_binary", train, test, optimize = TRUE)
# Fit and evaluate the performance of the XGBoost model
xg_fit <- boost_model("Diabetes_binary", train, test)
```

# Modeling analysis

## Logistic regression

According to the optimized logistic regression, we can see that several factors were marked as important in predicting diabetes. For example, a high blood pressure, high cholesterol, heart disease or attack, amongst others, though these had a high probability for the t-test removing the variable. Interestingly, we can see that all of the higher incomes were significant and the extent to which the significance increased with income. This suggests that income appears to scale inversely with the chance of type 2 diabetes. Another surprising figure is the coefficient for heavy alcohol consumption, -0.75. This coefficient is negative which means that patients who heavily consume alcohol are less likely to have been diagnosed with diabetes/pre-diabetes. A possible explanation for this could be that the variable is linked to sex and future upgrades to this package could include the option to add interaction variables.

The rest of the coefficient signs are fairly expected. The indicators for high blood pressure, high cholesterol, larger BMI, strokes, heart disease/attacks, and general health (larger value = worse health) contribute to diabetes. At the same time, eating veggies, having strong physical health and more education have negative coefficients contributing to decreasing in the likelihood of having diabetes.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
glm_fit <- glm_model("Diabetes_binary", diabetes, optimize = "manual")
```

```{r}
glm_fit <- glm_model("Diabetes_binary", train, test, optimize = "manual")
summary(glm_fit)
```

The following plots show three variables which were marked as highly significant, along with the BMI. The plot makes it clear that those with diabetes have a larger proportion of people with high blood pressure and high cholesterol than those without diabetes. Additionally, there are more men with diabetes than without and less women with diabetes than without. Their differences; however, are not very large and so it makes sense that the coefficient for sex in our logistic regression is not very large.

```{r, fig.width=7, fig.height=7}
p1 <- vis_2vars(diabetes, "Diabetes_binary", "HighBP")
p2 <- vis_2vars(diabetes, "Diabetes_binary", "HighChol")
p4 <- vis_2vars(diabetes, "Sex", "Diabetes_binary")
p3 <- vis_2vars(diabetes, "Diabetes_binary", "BMI")
gridExtra::grid.arrange(p1, p2, p3, p4)
```

## Random Forest

We ran the random forest model on the full data and chosen to optimize the model which means that we optimized the model to have the best mtry value. In other words, we chose the best tree split based on a random sample of an mtry number of predictors. Here that number is optimized to be 2 and the number of trees is 100. The error rate is 25.58% which means that it has 74.42% accuracy. For this model, we split the data into train and test and evaluated its performance. The accuracy for the test data was similar to that of the model, 74.84 %, which is not high for either case and the confidence interval is from 73.98% to 75.42%.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
rf_fit <- rf_model("Diabetes_binary", train, test, optimize = TRUE)
```

```{r}
print(rf_fit[[1]])
rf_fit[[6]]
```

The random forest model evaluated (according to the mean decrease in accuracy metric) BMI (30.82), general health (26.49), Age (24.75), blood pressure (22.16), and cholesterol (22.13) as the top five most important variables for categorizing patients between having and not having diabetes. It is unsurprising that both models would tie diabetes to general health and blood pressure as being important indicators similar to the glm. Meanwhile, the values that are least important are smoking (2.05), mental health (2.86), fruits (3.34), and veggies (3.22). This was also similar to the results from the previous model, where mental health had a small coefficient and both fruits and smoking were not significant enough to be included in the model.

The area under the curve here is 0.8186, which is not terrible and means that our model has a good level of separability between classes since it is close to 1.

```{r}
library(randomForest, include.only = "importance")
importance(rf_fit[[1]])
rf_fit[[7]]
```

## XG Boost
The training logloss is roughly 0.14 which is numerically small, however given that our predicted variable range is 0 to 1, it is relatively large for this model. The accuracy of 70.77% which means that both statistics agree the model is okay but not great at predicting diabetes.

```{r, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
xg_fit <- boost_model("Diabetes_binary", train, test)
```

```{r}
print(xg_fit[[1]])
print(xg_fit[[2]])
rf_fit[[7]]
```

Similar to the first two models, BMI(gain/contribution to the model):0.179, blood pressure: 0.176, age: 0.122, high cholesterol: 0.037 and general health: 0.028, are all important factors in predicting diabetes. Unlike the other models; however, age: 0.122 now seems to be higher in importance. This variable is unfortunately non-actionable in terms of lifestyle changes.
```{r}
print(xg_fit[[3]])
```


Overall, some indicators across models--like the doctor’s cost, fruits and veggies--aren’t as important as BMI, general health, blood pressure and cholesterol when predicting for diabetes. Also, variables like strokes and heart disease/attacks flip between the most and the least important depending on the model. Each of the three models have their own strengths for assessing feature relevance. The glm is the easiest to interpret, the random forests model is the most robust against outliers, and the xgboost model is the most powerful. Future analysis with these models will be dependent on the scope and purpose of the analysis.

# Counterfactual Analysis

The main purpose of this study is not only to understand which features are the most correlated with type 2 diabetes, but to provide a medical professional with important context about the *extent* that a certain feature does or does not put a patient at risk of diabetes. Therefore, we proceed with a counterfactual analysis on the following five features: `BMI` (patient's body mass index), `HeartDiseaseorAttack` (1 = patient has heart disease or history of heart attacks), `HighBP` (1 = patient has high blood pressure), `HighChol` (1 = patient has high cholesterol), and `PhysActivity` (0 = patient is not physically active). That is, we examine how changing these features from unhealthy to healthy levels changes each model's predicted proportion of diabetes (among the diabetic subset of the test data).

We focus on five these features in particular because patient intervention is possible. For example, if the patient *does* have a history of poor heart health, or is *not* physically active, then a doctor would be able to directly mitigate a person's risk for type 2 diabetes by prescribing appropriate dietary and/or exercise regimens, respectively. On the other hand, we intentionally exclude factors such as `GenHlth` (general health) and `Income` (financial income), where the former is a feature that is measured on a subjective scale, and the latter is a feature over which the patient has relatively less agency. Finally, note that for a fair counterfactual analysis across all three models, we were restricted to variables that were statistically significant according to the glm model, otherwise we would not be able to test how different feature levels affect risk of diabetes.

Again, to minimize model bias, the counterfactual analysis is conducted on the diabetic subset of the test data, not the training data. For each of the five features, we record each model's predicted proportion of diabetes before and after adjusting the features to healthy levels. Importantly, this analysis is a highly interpretable method for comparing the relative impact that each feature has on a patient's likelihood of diabetes. For BMI, we threshold all patients into a range of 15 to 17, which are ubiquitously healthy levels (within the 5th to 85th percentile, according to the [CDC](https://www.cdc.gov/healthyweight/assessing/bmi/childrens_bmi/about_childrens_bmi.html)) for the ages included in this study (13 years old and younger). For the other four binary categorical features, we simply adjust each from the unhealthy to healthy level.

```{r}
# xg boost counterfactual
get_counterfactual(model = xg_fit[[1]], data = test)
```

```{r}
# random forest counterfactual
get_counterfactual(model = rf_fit[[1]], data = test)
```

```{r}
# glm counterfactual
get_counterfactual(model = glm_fit[[1]], data = test)
```

In the three counterfactual tables above, the "Before" and "After" columns represent the predicted proportion of diabetes among the diabetic subset of the test data before/after adjusting each feature. Notice that the proportions in the "Before" column measure how well each model correctly detects diabetes, because we are only considering individuals with diabetes. In other words, in the "Before" column, the ideal model predicts all 1s, so values closer to 0 correspond to higher probabilities of false negatives (i.e., type 2 error) for the subset corresponding to that row. For example, while the random forest counterfactual suggests that decreasing `BMI` to the CDC recommended range of 15-17 decreases the probability of diabetes by about 15\%, the validity of this result is offset by the relatively higher type 2 error (about 20\% of the test data with diabetes was misclassified as non diabetic). Lastly, note that it makes sense that all values in the "Change" column are negative, because we are adjusting each feature from unhealthy to healthy levels, so we should expect the diabetic proportion to decrease. 

Overall, taking into consideration these type 2 error rates, the three models generally agree that adjusting `BMI` and `HighBP` to healthy levels were the most effective for decreasing the probability of diabetes. Furthermore, while `HeartDiseaseorAttack` was ubiquitously the best at predicting the presence of diabetes (i.e., had the lowest type 2 error rate), changing this factor to the healthy level led to a negligible decrease in the probability of diabetes. On the other hand, `HighBP` had comparable type 2 error rates to `HeartDiseaseorAttack` for the three models, *and* adjusting the factor led to comparatively greater decreases in the probability of diabetes. Finally, we are very surprised to find that, in the logistic (glm) model, there was no change in the probability of diabetes after adjusting `PhysActivity` to a healthy level! This is a surprising finding, given that the factor was a statistically significant variable in the logistic model.

Returning to the motivating hypothesis for our study, in which we are interested in finding which features are simultaneously practical (in the sense that intervention for a patient is possible) *and* highly relevant to detecting a patient's risk of diabetes, the counterfactual analysis shows a strong consensus for blood pressure and BMI being useful proxies for a patient's risk of diabetes. The findings from our counterfactual analysis completely recontextualize the previous statistical modeling, showing that statistical significance does not necessarily translate to real-world significance for patient intervention.
